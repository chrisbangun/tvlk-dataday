{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are RNNs?\n",
    "\n",
    "![alt text](rnn.jpg \"RNN\")\n",
    "\n",
    "The above diagram shows a RNN being unrolled (or unfolded) into a full network. By unrolling we simply mean that we write out the network for the complete sequence. For example, if the sequence we care about is a sentence of 5 words, the network would be unrolled into a 5-layer neural network, one layer for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Ideas Behind RNN\n",
    "\n",
    "1. Sequential Data: Dependency for every data within every time steps\n",
    "2. Shared Weights: Using the same parameters accross all steps and makes it possible to generalize the model despite of the difference in length for every examples\n",
    "\n",
    "![alt text](rnns.jpeg \"rnns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla RNN\n",
    "\n",
    "![alt text](vanilla-rnn.png \"Vanilla RNN\")\n",
    "\n",
    "Where __st__ is the memory of RNN, __xt__ is the input for current time step, __ot__ is the output for current time step, and __U__, __V__, __W__ represent weights for input, output and memory respectively.\n",
    "\n",
    "Based on the above formula, it introduces two major advantages:\n",
    "1. Regardless of the sequence length, the learned model always has the same input size (in vectors)\n",
    "2. It is possible to use the same transition function to calculate curernt memory with the same parameters at every time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train them?\n",
    "\n",
    "## Backpropagation Through Time\n",
    "\n",
    "![alt text](bptt.png \"Backpropagation Through Time\")\n",
    "\n",
    "As we may recall, we have several parameters to train, __W__, __U__, and __V__. For the rest of this section, we will use __E3__ as an example.\n",
    "\n",
    "For each time steps, we calculate the following:\n",
    "1. ![alt text](dl-dv.png \"dl-dv\")\n",
    "2. ![alt text](dl-dw1.png \"dl-dw1\") ![alt text](dl-dw2.png \"dl-dw2\")\n",
    "3. ![alt text](dl-du.png \"dl-du\")\n",
    "\n",
    "Where:\n",
    "1. ![alt text](E.png \"E\")\n",
    "2. ![alt text](y.png \"y\")\n",
    "3. ![alt text](q.png \"q\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-Term Dependencies Problem\n",
    "\n",
    "![alt text](dl-dw2 problems.png \"dl-dw2\")\n",
    "\n",
    "The red square shows the problem while doing bptt in RNN. Depends on the weights, if they are to small the the gradient will __vanish__. On the other hand, if the weights are too big, it will __explode__\n",
    "\n",
    "## How to mitigate?\n",
    "\n",
    "1. For exploding gradient, a simple type of solution has been in use by practitioners for many years is __clipping the gradient__\n",
    "![alt text](gradient clipping pseudocode.png \"gradient clipping pseudocode\")\n",
    "![alt text](gradient clipping effect.png \"gradient clipping effect\")\n",
    "2. Adjust weights initialization\n",
    "3. Change activation function (from sigmoid to relu, tanh, leaky relu, etc)\n",
    "4. Batch Normalization\n",
    "5. Use other cell, such as LSTM or GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "The problem we are trying to solve in this implementation is Language Modeling. Language model is the art of determining the probability of a sequence of words. In layman's terms, we are trying teach the machine how to generate words based on their probability of occurence given a pair of words.\n",
    "\n",
    "## Requirements\n",
    "1. [Install PyTorch](http://pytorch.org/)\n",
    "2. Install torchtext by running this command ```pip install git+https://github.com/pytorch/text --upgrade```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "Let's start by loading data using torch text. For this example we will use provided WikiText-2 data from torch text. The WikiText language modeling dataset is a collection of over 100 million tokens extracted from the set of verified Good and Featured articles on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "BPTT_LEN = 30\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = datasets.WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors=GloVe(name=\"6B\", dim=EMBEDDING_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 130768), (',', 99913), ('.', 73388), ('of', 57030), ('<unk>', 54625)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test), batch_size=BATCH_SIZE, bptt_len=BPTT_LEN,\n",
    "    device=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the sample of how our dataset looks like. It consists of sequence of words, from which we will try to predict what is the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<eos> = valkyria chronicles iii = <eos> <eos> senjō no valkyria 3 : <unk> chronicles ( japanese : 戦場のヴァルキュリア3 , lit . valkyria of the battlefield 3 ) , commonly for its release model in contrast to the rock band series , causing some players to hold contempt towards activision . harmonix considered the rock band series as a \" top rope against both headshrinkers . as he tried to attack samu from the top rope again , samu caught him and <unk> him before fatu executed a diving splash\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_iter))\n",
    "data = batch.text.transpose(1, 0).data.numpy()\n",
    "sample = []\n",
    "for d1 in data:\n",
    "    for d2 in d1:\n",
    "        sample.append(TEXT.vocab.itos[d2])\n",
    "print(\" \".join(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training Data: 23207\n",
      "Total Validation Data: 23207\n",
      "Total Testing Data: 23207\n",
      "Total Vocabularies: 28913\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Training Data:\", len(train_iter))\n",
    "print(\"Total Validation Data:\", len(train_iter))\n",
    "print(\"Total Testing Data:\", len(train_iter))\n",
    "print(\"Total Vocabularies:\", len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss [0/23207]: 10.284704\n",
      "Loss [100/23207]: 7.636081\n",
      "Loss [200/23207]: 7.254285\n",
      "Loss [300/23207]: 7.189576\n",
      "Loss [400/23207]: 7.070719\n",
      "Loss [500/23207]: 6.672390\n",
      "Loss [600/23207]: 6.892480\n",
      "Loss [700/23207]: 6.824963\n",
      "Loss [800/23207]: 6.405207\n",
      "Loss [900/23207]: 6.652770\n",
      "Loss [1000/23207]: 7.195287\n",
      "Loss [1100/23207]: 6.251607\n",
      "Loss [1200/23207]: 6.896472\n",
      "Loss [1300/23207]: 6.249759\n",
      "Loss [1400/23207]: 6.997591\n",
      "Loss [1500/23207]: 6.630477\n",
      "Loss [1600/23207]: 6.002308\n",
      "Loss [1700/23207]: 6.442147\n",
      "Loss [1800/23207]: 7.107758\n",
      "Loss [1900/23207]: 6.655063\n",
      "Loss [2000/23207]: 6.274049\n",
      "Loss [2100/23207]: 6.689747\n",
      "Loss [2200/23207]: 5.470231\n",
      "Loss [2300/23207]: 6.647050\n",
      "Loss [2400/23207]: 6.639992\n",
      "Loss [2500/23207]: 6.521108\n",
      "Loss [2600/23207]: 6.296050\n",
      "Loss [2700/23207]: 6.597279\n",
      "Loss [2800/23207]: 6.092088\n",
      "Loss [2900/23207]: 6.122556\n",
      "Loss [3000/23207]: 6.447164\n",
      "Loss [3100/23207]: 5.756624\n",
      "Loss [3200/23207]: 5.866466\n",
      "Loss [3300/23207]: 6.300177\n",
      "Loss [3400/23207]: 6.252677\n",
      "Loss [3500/23207]: 6.046329\n",
      "Loss [3600/23207]: 6.485835\n",
      "Loss [3700/23207]: 6.641669\n",
      "Loss [3800/23207]: 6.358695\n",
      "Loss [3900/23207]: 5.754786\n",
      "Loss [4000/23207]: 6.315854\n",
      "Loss [4100/23207]: 6.413792\n",
      "Loss [4200/23207]: 6.646234\n",
      "Loss [4300/23207]: 5.985898\n",
      "Loss [4400/23207]: 6.100800\n",
      "Loss [4500/23207]: 6.755691\n",
      "Loss [4600/23207]: 5.914700\n",
      "Loss [4700/23207]: 6.084371\n",
      "Loss [4800/23207]: 6.135201\n",
      "Loss [4900/23207]: 7.393200\n",
      "Loss [5000/23207]: 6.251674\n",
      "Loss [5100/23207]: 5.987143\n",
      "Loss [5200/23207]: 5.625259\n",
      "Loss [5300/23207]: 6.326581\n",
      "Loss [5400/23207]: 6.426130\n",
      "Loss [5500/23207]: 5.976098\n",
      "Loss [5600/23207]: 6.312148\n",
      "Loss [5700/23207]: 6.550511\n",
      "Loss [5800/23207]: 6.630642\n",
      "Loss [5900/23207]: 6.312495\n",
      "Loss [6000/23207]: 6.223853\n",
      "Loss [6100/23207]: 6.165797\n",
      "Loss [6200/23207]: 6.250859\n",
      "Loss [6300/23207]: 5.492705\n",
      "Loss [6400/23207]: 6.336570\n",
      "Loss [6500/23207]: 6.214839\n",
      "Loss [6600/23207]: 5.695576\n",
      "Loss [6700/23207]: 5.725412\n",
      "Loss [6800/23207]: 5.621624\n",
      "Loss [6900/23207]: 5.374771\n",
      "Loss [7000/23207]: 5.833947\n",
      "Loss [7100/23207]: 5.783076\n",
      "Loss [7200/23207]: 5.473538\n",
      "Loss [7300/23207]: 6.011854\n",
      "Loss [7400/23207]: 5.757659\n",
      "Loss [7500/23207]: 6.543799\n",
      "Loss [7600/23207]: 6.437039\n",
      "Loss [7700/23207]: 5.758174\n",
      "Loss [7800/23207]: 5.963047\n",
      "Loss [7900/23207]: 6.358995\n",
      "Loss [8000/23207]: 6.123761\n",
      "Loss [8100/23207]: 6.231101\n",
      "Loss [8200/23207]: 6.416083\n",
      "Loss [8300/23207]: 6.344241\n",
      "Loss [8400/23207]: 5.417987\n",
      "Loss [8500/23207]: 6.292472\n",
      "Loss [8600/23207]: 6.432411\n",
      "Loss [8700/23207]: 5.727255\n",
      "Loss [8800/23207]: 5.980451\n",
      "Loss [8900/23207]: 5.253411\n",
      "Loss [9000/23207]: 6.877376\n",
      "Loss [9100/23207]: 6.597704\n",
      "Loss [9200/23207]: 5.565670\n",
      "Loss [9300/23207]: 6.522633\n",
      "Loss [9400/23207]: 5.823084\n",
      "Loss [9500/23207]: 6.364663\n",
      "Loss [9600/23207]: 6.107584\n",
      "Loss [9700/23207]: 6.277872\n",
      "Loss [9800/23207]: 6.111169\n",
      "Loss [9900/23207]: 6.188980\n",
      "Loss [10000/23207]: 6.291633\n",
      "Loss [10100/23207]: 5.240891\n",
      "Loss [10200/23207]: 6.009214\n",
      "Loss [10300/23207]: 6.039042\n",
      "Loss [10400/23207]: 5.868214\n",
      "Loss [10500/23207]: 6.005994\n",
      "Loss [10600/23207]: 5.438363\n",
      "Loss [10700/23207]: 6.989020\n",
      "Loss [10800/23207]: 5.989802\n",
      "Loss [10900/23207]: 5.663369\n",
      "Loss [11000/23207]: 6.104077\n",
      "Loss [11100/23207]: 6.335086\n",
      "Loss [11200/23207]: 6.089396\n",
      "Loss [11300/23207]: 5.966474\n",
      "Loss [11400/23207]: 5.670220\n",
      "Loss [11500/23207]: 5.053884\n",
      "Loss [11600/23207]: 5.947148\n",
      "Loss [11700/23207]: 6.163622\n",
      "Loss [11800/23207]: 5.821736\n",
      "Loss [11900/23207]: 5.736329\n",
      "Loss [12000/23207]: 5.171558\n",
      "Loss [12100/23207]: 5.366939\n",
      "Loss [12200/23207]: 5.452491\n",
      "Loss [12300/23207]: 6.132007\n",
      "Loss [12400/23207]: 6.531055\n",
      "Loss [12500/23207]: 5.854489\n",
      "Loss [12600/23207]: 5.880636\n",
      "Loss [12700/23207]: 5.408738\n",
      "Loss [12800/23207]: 5.444955\n",
      "Loss [12900/23207]: 5.740613\n",
      "Loss [13000/23207]: 6.437211\n",
      "Loss [13100/23207]: 6.173409\n",
      "Loss [13200/23207]: 6.397791\n",
      "Loss [13300/23207]: 5.847715\n",
      "Loss [13400/23207]: 6.418876\n",
      "Loss [13500/23207]: 5.730371\n",
      "Loss [13600/23207]: 5.545626\n",
      "Loss [13700/23207]: 4.810660\n",
      "Loss [13800/23207]: 5.882865\n",
      "Loss [13900/23207]: 4.960001\n",
      "Loss [14000/23207]: 5.786011\n",
      "Loss [14100/23207]: 5.896185\n",
      "Loss [14200/23207]: 5.984169\n",
      "Loss [14300/23207]: 5.837677\n",
      "Loss [14400/23207]: 6.073580\n",
      "Loss [14500/23207]: 6.195768\n",
      "Loss [14600/23207]: 5.772466\n",
      "Loss [14700/23207]: 5.697379\n",
      "Loss [14800/23207]: 5.380915\n",
      "Loss [14900/23207]: 5.871031\n",
      "Loss [15000/23207]: 5.660615\n",
      "Loss [15100/23207]: 5.179881\n",
      "Loss [15200/23207]: 5.654695\n",
      "Loss [15300/23207]: 5.930394\n",
      "Loss [15400/23207]: 6.006060\n",
      "Loss [15500/23207]: 6.015329\n",
      "Loss [15600/23207]: 6.251213\n",
      "Loss [15700/23207]: 5.599257\n",
      "Loss [15800/23207]: 5.179716\n",
      "Loss [15900/23207]: 5.089033\n",
      "Loss [16000/23207]: 5.001385\n",
      "Loss [16100/23207]: 5.927980\n",
      "Loss [16200/23207]: 5.787673\n",
      "Loss [16300/23207]: 5.252838\n",
      "Loss [16400/23207]: 5.290843\n",
      "Loss [16500/23207]: 5.160118\n",
      "Loss [16600/23207]: 5.647966\n",
      "Loss [16700/23207]: 6.554308\n",
      "Loss [16800/23207]: 5.105309\n",
      "Loss [16900/23207]: 5.523946\n",
      "Loss [17000/23207]: 6.036705\n",
      "Loss [17100/23207]: 5.241342\n",
      "Loss [17200/23207]: 5.718350\n",
      "Loss [17300/23207]: 4.783609\n",
      "Loss [17400/23207]: 5.821529\n",
      "Loss [17500/23207]: 5.763174\n",
      "Loss [17600/23207]: 5.868617\n",
      "Loss [17700/23207]: 5.263766\n",
      "Loss [17800/23207]: 5.122537\n",
      "Loss [17900/23207]: 5.630454\n",
      "Loss [18000/23207]: 5.972828\n",
      "Loss [18100/23207]: 6.386398\n",
      "Loss [18200/23207]: 5.619121\n",
      "Loss [18300/23207]: 5.594449\n",
      "Loss [18400/23207]: 5.715289\n",
      "Loss [18500/23207]: 6.285222\n",
      "Loss [18600/23207]: 5.522757\n",
      "Loss [18700/23207]: 6.284220\n",
      "Loss [18800/23207]: 4.908783\n",
      "Loss [18900/23207]: 5.687134\n",
      "Loss [19000/23207]: 6.496545\n",
      "Loss [19100/23207]: 5.247671\n",
      "Loss [19200/23207]: 5.160926\n",
      "Loss [19300/23207]: 4.926049\n",
      "Loss [19400/23207]: 5.512008\n",
      "Loss [19500/23207]: 5.771882\n",
      "Loss [19600/23207]: 6.129867\n",
      "Loss [19700/23207]: 6.179472\n",
      "Loss [19800/23207]: 5.138536\n",
      "Loss [19900/23207]: 5.502544\n",
      "Loss [20000/23207]: 5.656421\n",
      "Loss [20100/23207]: 5.769666\n",
      "Loss [20200/23207]: 5.055944\n",
      "Loss [20300/23207]: 5.663976\n",
      "Loss [20400/23207]: 5.228907\n",
      "Loss [20500/23207]: 5.936554\n",
      "Loss [20600/23207]: 5.754885\n",
      "Loss [20700/23207]: 6.249555\n",
      "Loss [20800/23207]: 5.084359\n",
      "Loss [20900/23207]: 5.156173\n",
      "Loss [21000/23207]: 5.207355\n",
      "Loss [21100/23207]: 5.952139\n",
      "Loss [21200/23207]: 5.665560\n",
      "Loss [21300/23207]: 6.694343\n",
      "Loss [21400/23207]: 5.948405\n",
      "Loss [21500/23207]: 5.714606\n",
      "Loss [21600/23207]: 5.623393\n",
      "Loss [21700/23207]: 5.556263\n",
      "Loss [21800/23207]: 5.065580\n",
      "Loss [21900/23207]: 6.661852\n",
      "Loss [22000/23207]: 5.136291\n",
      "Loss [22100/23207]: 6.149017\n",
      "Loss [22200/23207]: 6.020156\n",
      "Loss [22300/23207]: 5.578479\n",
      "Loss [22400/23207]: 6.699769\n",
      "Loss [22500/23207]: 5.506301\n",
      "Loss [22600/23207]: 6.196606\n",
      "Loss [22700/23207]: 6.137573\n",
      "Loss [22800/23207]: 5.957271\n",
      "Loss [22900/23207]: 5.220524\n",
      "Loss [23000/23207]: 5.727759\n",
      "Loss [23100/23207]: 6.121997\n",
      "Loss [23200/23207]: 5.642556\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "############################\n",
    "# Variable Initialization #\n",
    "############################\n",
    "HIDDEN_SIZE = 100\n",
    "NUM_LAYERS = 1\n",
    "DROPOUT = 0.5\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "\n",
    "#################################\n",
    "# Neural Network Initialization #\n",
    "#################################\n",
    "class LanguageModelLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LanguageModelLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=EMBEDDING_DIM,\n",
    "                            hidden_size=HIDDEN_SIZE,\n",
    "                            num_layers=NUM_LAYERS,\n",
    "                            dropout=DROPOUT)\n",
    "        self.linear = nn.Linear(in_features=HIDDEN_SIZE,\n",
    "                                out_features=VOCAB_SIZE)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        lstm_out, lstm_hidden = self.lstm(X)\n",
    "        step_size, batch_size = lstm_out.size()\n",
    "        modified_output = lstm_out.view(step_size * batch_size, -1)\n",
    "        \n",
    "        is_tensor_equal = (lstm_out[0][0] == modified_output[0])\n",
    "        assert (is_tensor_equal.sum() == HIDDEN_SIZE).data.numpy()[0]\n",
    "        out = self.linear(modified_output)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding = nn.Embedding(TEXT.vocab.vectors.size(0),\n",
    "                         TEXT.vocab.vectors.size(1))\n",
    "embedding.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model = LanguageModelLSTM()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "opt = Adam(model.parameters())\n",
    "\n",
    "################\n",
    "# RNN Training #\n",
    "################\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "total_steps = len(train_iter)\n",
    "for idx, batch in enumerate(train_iter):\n",
    "    model.zero_grad()\n",
    "    word_embedding = embedding(batch.text)\n",
    "    out = model(word_embedding)\n",
    "    \n",
    "    target = batch.target.view(BPTT_LEN * BATCH_SIZE)  \n",
    "    loss = loss_fn(out, target)\n",
    "    \n",
    "    break\n",
    "    if idx % 100 == 0:\n",
    "        print(\"Loss [%d/%d]: %f\" % (idx, total_steps,\n",
    "                                    loss.data.numpy()[0]))\n",
    "\n",
    "    loss.backward()\n",
    "    \n",
    "    opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 28913\n",
      "PREDICTION: \n",
      ", <unk> the = of <unk> shelley be , , <unk> the = . the <eos> . was the been the <unk> the player , by <unk> a be \" <unk> <unk> \" <unk> game , , <unk> , and . . series . . <unk> <unk> of , ) . , . the <eos> and own was the , a of . <unk> in the for the 's in games that and . the , . s the <unk> \" guitar @-@ \" series . \" . . the \n",
      "\n",
      "REAL LABEL: \n",
      "<eos> = robert <unk> = <eos> <eos> robert <unk> is an english film , television and theatre actor . he had a guest @-@ starring role on the television series the rights to the <unk> shield had been sold to a new australian baseball league ( <unk> ) , with ownership split between major league baseball 's 75 percent share and bright \" and that he evoked the \" delicate , <unk> power @-@ playing vignettes \" of his theater work . jackson said <unk> ' theatrical roots rarely showed 90 28913\n",
      "PREDICTION: \n",
      "<unk> the and of <unk> the the @-@ the . of common <eos> of <unk> was in common also the of by <unk> , the government of <unk> <unk> \" <unk> . \" in <eos> 's the assembly <unk> series was , , , not . released be by a the the for game <unk> the . , game <eos> and championship was was championship that released . the in to the the the in . . series the 2009 to time , <unk> navy <eos> , , was . \n",
      "\n",
      "REAL LABEL: \n",
      "the bill in 2000 . this was followed by a starring role in the play herons written by simon stephens , which was performed in 2001 at the royal court and the 25 percent share owned by the australian baseball federation . the 2010 tournament was considered preparation for the inaugural <unk> season starting in 2010 – 11 . it , and that the \" most remarkable \" aspect was that spacey 's performance did not overshadow the film . he said that <unk> worked the script 's <unk> smoothly 90 28913\n",
      "PREDICTION: \n",
      ", of and <eos> the the was <unk> <unk> been . , <unk> <unk> <unk> for , and in the and the the the series <unk> in series . series . the of . the . <unk> games . , . <eos> the <eos> 's . the that the interview <unk> the , , , of <unk> the <unk> <unk> @-@ @-@ \" number is \" in of the a also \" the in , <unk> and the , of , the were <unk> guitar with , of the <unk> \n",
      "\n",
      "REAL LABEL: \n",
      "theatre . he had a guest role in the television series judge john <unk> in 2002 . in 2004 <unk> landed a role as \" craig \" in the episode varied from the 2009 <unk> shield by expanding the season to include ten rounds . since an uneven number ( five ) teams were involved , four teams paired off , to the ensemble 's strengths , and staged the tonal shifts <unk> . mccarthy believed american beauty a \" stunning card of introduction \" for film <unk> <unk> and 90 28913\n",
      "PREDICTION: \n",
      "<unk> the . <unk> other <eos> <unk> of was \" the that , with the the new s <unk> @-@ , series year that , . \" series , . @-@ and \" . the <unk> the guitar \" was of \" in was \" the a by in match <unk> <unk> @-@ \" <unk> <eos> own <unk> the , <unk> series . , , the <eos> and 's was other that released was 's in a death the players of series @-@ the – , , , the the \n",
      "\n",
      "REAL LABEL: \n",
      "\" teddy 's story \" of the television series the long firm ; he starred alongside actors mark strong and derek jacobi . he was cast in the 2005 theatre for each round and played a three @-@ game series , while the remaining team took a bye . during the season , each team had two bye rounds and ball . he said <unk> ' \" sure hand \" was \" as precise and controlled \" as his theater work . mccarthy cited hall 's involvement as fortunate for 90 28913\n",
      "PREDICTION: \n",
      ". with , the of and series . well , the <unk> , other of , other a . . <unk> and and \" and of , was the the released . \" in the <unk> the time of time . the , <eos> guitar . the of the , the . the <eos> the assembly to series was the . the the . the s . @-@ , the players the . to , <eos> , a was , <unk> a game \" by , , the the and \n",
      "\n",
      "REAL LABEL: \n",
      "productions of the philip ridley play mercury fur , which was performed at the drum theatre in plymouth and the <unk> <unk> factory in london . he was directed by played two rounds against each other team , one at home and one away . in total , the schedule allowed for 24 regular @-@ season games per team before <unk> , as the cinematographer was \" <unk> \" at conveying the themes of a work . <unk> agreed that <unk> ' choice of collaborators was \" <unk> \" , "
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(test_iter):\n",
    "    word_embedding = embedding(batch.text)\n",
    "    out = model(word_embedding)\n",
    "    values, indices = out.max(1)\n",
    "    \n",
    "    print(\"PREDICTION: \")\n",
    "    for idx in indices.data.numpy():\n",
    "        print(TEXT.vocab.itos[idx], end=\" \")\n",
    "    print(\"\\n\\nREAL LABEL: \")\n",
    "    for idx in batch.text.transpose(1, 0).data.numpy():\n",
    "        for idx2 in idx:\n",
    "            print(TEXT.vocab.itos[idx2], end=\" \")\n",
    "            \n",
    "    if (i+1) % 5 == 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
